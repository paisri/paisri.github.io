---
layout: page
title: Publications
---

# Academic Publications

- [Robustness to fundamental uncertainty in AGI alignment](https://arxiv.org/abs/1807.09836), appearing in [Journal of Consciousness Studies, Volume 27, Numbers 1-2, 2019, pp. 225-241(17)](https://www.ingentaconnect.com/openurl?genre=article&issn=1355-8250&volume=27&issue=1*&spage=225&epage=241&aulast=Worley+III)

# Technical Communications

- [Goodhart's Curse and Limitations on AI Alignment](https://www.lesswrong.com/posts/NqQxTn5MKEYhSnbuB/goodhart-s-curse-and-limitations-on-ai-alignment)
- [A developmentally-situated approach to teaching normative behavior to AI](https://www.lesswrong.com/posts/uEAvtbtEBdsQJMdh8/a-developmentally-situated-approach-to-teaching-normative) - Winner of the [EthicsNet Guardians' Challenge](https://www.herox.com/EthicsNet/community)
- [Avoiding AGI races through self-regulation](https://www.preprints.org/manuscript/201810.0030/v2) ([general audience version](https://mapandterritory.org/avoiding-ai-races-through-self-regulation-1b815fca6b06)) - Runner up to the [Solving the AI Race General AI Challenge](https://medium.com/goodai-news/solving-the-ai-race-finalists-15-000-of-prizes-5f57d1f6a45f)
- [Formally Stating the AI Alignment Problem](https://mapandterritory.org/formally-stating-the-ai-alignment-problem-fe7a6e3e5991)

# Research Notes

- [Formal Alignment Introduction](https://www.lesswrong.com/s/sv2CwqTCso8wDdmmi)
  - [Towards deconfusing values](https://www.lesswrong.com/posts/WAqG5BQMzAs34mpc2/towards-deconfusing-values)
  - [Values, Valence, and Alignment](https://www.lesswrong.com/posts/ALvnz3DrjHwmLG29F/values-valence-and-alignment)
  - [Minimization of prediction error as a foundation for human values in AI alignment](https://www.lesswrong.com/posts/Cu7yv4eM6dCeA67Af/minimization-of-prediction-error-as-a-foundation-for-human)
- [TAISU 2019 Field Report](https://www.lesswrong.com/posts/MmX2ZqET2QDYpSMDp/taisu-2019-field-report)
- [Let Values Drift](https://www.lesswrong.com/posts/JYdPbGS9mpJn3SAyA/let-values-drift-1)
- [HLAI 2018 Field Report](https://www.lesswrong.com/posts/axsizR4vEX8qtuLpR/hlai-2018-field-report)
- [Safety in Machine Learning](https://www.lesswrong.com/posts/3iP8P57mNpHBFfYkd/safety-in-machine-learning)
- [Thoughts on "AI Safety via Debate"](https://www.lesswrong.com/posts/WRy6KNnxwQHc5Ktjc/thoughts-on-ai-safety-via-debate)
- [How safe "safe" AI development?](https://www.lesswrong.com/posts/JDZsoykx3KBp8ptEi/how-safe-safe-ai-development)
- [Phenomenological AI Alignment Introduction](https://www.lesswrong.com/s/CRvxidrCkp7YE7gSK)
  - [Computational Complexity of P-Zombies](https://mapandterritory.org/computational-complexity-of-p-zombies-fc56909af96f)
  - [AI Alignment and Phenomenal Consciousness](https://mapandterritory.org/ai-alignment-and-phenomenal-consciousness-2ca23de6aebd)
  - [Towards and Axiological Approach to AI Alignment](https://mapandterritory.org/towards-an-axiological-approach-to-ai-alignment-4993d044d1b8)
